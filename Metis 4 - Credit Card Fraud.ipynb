{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29574a76",
   "metadata": {},
   "source": [
    "# Libraries / Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0640dec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're pitching this to banks companies\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "import time\n",
    "from time import sleep, time\n",
    "from timeit import timeit\n",
    "\n",
    "import random\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sklearn as sk\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import pickle\n",
    "\n",
    "import imblearn\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf90722d",
   "metadata": {},
   "source": [
    "# General-use Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab9e0e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reads csv file and returns X and y arrays/dataframes\n",
    "def read_dataset(as_numpy_array=True):\n",
    "    read_data_=pd.read_csv(\"creditcard.csv\")\n",
    "    X=read_data_.drop(['Class'],axis=1)\n",
    "    y=read_data_['Class']\n",
    "    del read_data_\n",
    "    if as_numpy_array:\n",
    "        return(np.array(X),np.array(y))\n",
    "    else:\n",
    "        return(X,y)\n",
    "    \n",
    "def read_dataset_reg(as_numpy_array=True):\n",
    "    read_data_=pd.read_csv(\"creditcard.csv\")\n",
    "    scaler = StandardScaler()\n",
    "    y=read_data_['Class']\n",
    "    read_data_.drop(['Class'],axis=1,inplace=True)\n",
    "    X = scaler.fit_transform(read_data_)\n",
    "\n",
    "    del read_data_\n",
    "    if as_numpy_array:\n",
    "        return(np.array(X),np.array(y))\n",
    "    else:\n",
    "        return(X,y)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bd98f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scoring Models from Class. Eval. Pair\n",
    "def accuracy(actuals, preds):\n",
    "    truth_matrix=actuals==preds\n",
    "    success=0\n",
    "    for item in truth_matrix:\n",
    "        if item:\n",
    "            success+=1\n",
    "    return(success/len(actuals))\n",
    "\n",
    "\n",
    "def precision(actuals, preds):\n",
    "    true_positive=0\n",
    "    false_positive=0\n",
    "    for i in range(len(actuals)):\n",
    "        if not actuals[i] and preds[i]:\n",
    "            false_positive+=1\n",
    "        if actuals[i] and preds[i]:\n",
    "            true_positive+=1\n",
    "            \n",
    "    if true_positive+false_positive==0:\n",
    "        return('error: divide by 0')\n",
    "    return(true_positive/(true_positive+false_positive))\n",
    "    \n",
    "    \n",
    "def recall(actuals, preds):\n",
    "    true_positive=0\n",
    "    false_negative=0\n",
    "    for i in range(len(actuals)):\n",
    "        if actuals[i] and not preds[i]:\n",
    "            false_negative+=1\n",
    "        if actuals[i] and preds[i]:\n",
    "            true_positive+=1\n",
    "            \n",
    "    if true_positive+false_negative==0:\n",
    "        return('error: divide by 0')\n",
    "    return(true_positive/(true_positive+false_negative))\n",
    "\n",
    "\n",
    "def f1(actuals, preds):\n",
    "    if precision(actuals,preds)=='error: divide by 0':\n",
    "        return('precision error: divide by 0')\n",
    "    if recall(actuals,preds)=='error: divide by 0':\n",
    "        return('recall error: divide by 0')\n",
    "    \n",
    "    mult=precision(actuals,preds)*recall(actuals,preds)\n",
    "    add=precision(actuals,preds)+recall(actuals,preds)\n",
    "    return(2*mult/add)\n",
    "\n",
    "def f1v2(precision, recall):\n",
    "    mult=precision*recall\n",
    "    add=precision+recall\n",
    "    return(2*mult/add)\n",
    "    \n",
    "def score(actuals, preds):\n",
    "    \n",
    "    return({\"accuracy\":accuracy(actuals, preds), \"precision\":precision(actuals, preds), \"recall\":recall(actuals, preds), \"f1\":f1(actuals, preds)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ed23230",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Currency value in Euros, but the model can be used for any currency/location\n",
    "\n",
    "TotalFraudCost=958*1e6\n",
    "TotalFraudCards=11.29*1e6\n",
    "\n",
    "AvgFraudLoss=TotalFraudCost/TotalFraudCards\n",
    "AvgFraudLoss\n",
    "\n",
    "#Cost of investigation\n",
    "CpBase=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ffee063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the sake of simplicity, we assume that customers do not churn due to fraud,\n",
    "# but we'll include the variables in the cost function for ease of re-fitting models.\n",
    "\n",
    "#Placeholder variables:\n",
    "#Chance of churn becuase of False Neg P(Cn)\n",
    "PCn=0.00\n",
    "#Chance of churn becuase of False Pos = P(Cp)\n",
    "PCp=0.00\n",
    "#Cost of replacing customer CR\n",
    "CR=0\n",
    "\n",
    "Cn=CR*PCn\n",
    "Cp=CR*PCp\n",
    "\n",
    "\n",
    "#Calculates costs per transaction\n",
    "def costs(actuals, preds, f_pos_cost=CpBase+Cp,f_neg_cost=AvgFraudLoss+Cn):\n",
    "    pred_positive=0\n",
    "    false_negative=0\n",
    "    \n",
    "    for i in range(len(actuals)):\n",
    "        #We have to pay for investigations regardless of whether transaction is really fraudulent\n",
    "        if preds[i]:\n",
    "            pred_positive+=1\n",
    "        #We lose money only if fraud not intercepted\n",
    "        if actuals[i] and not preds[i]:\n",
    "            false_negative+=1\n",
    "    \n",
    "    total_cost=f_pos_cost*pred_positive+f_neg_cost*false_negative\n",
    "    return(total_cost/len(actuals))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db3d9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline costs for naive models\n",
    "y=read_dataset()[1]\n",
    "all_legit_costs=costs(y,np.zeros(len(y)))\n",
    "all_fraud_costs=costs(y,np.ones(len(y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4e3d6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost per Transaction if all marked legitimate: €0.14658381170363086\n",
      "Cost per Transaction if all marked fraudulent: €6.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Cost per Transaction if all marked legitimate: €{all_legit_costs}')\n",
    "print(f'Cost per Transaction if all marked fraudulent: €{all_fraud_costs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc59fd07",
   "metadata": {},
   "source": [
    "# Prepare Dataset for out-of-box models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78c08b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data, split it 60/40 Train/Test\n",
    "\n",
    "Xy=read_dataset()\n",
    "X_train, X_test, y_train, y_test=train_test_split(Xy[0],Xy[1],test_size=0.2,random_state=hash(\"Server-Clearing Market Gardener\")%(2**32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71959e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraud_count=(y_train==1).sum()\n",
    "test_fraud_count=(y_test==1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eb521d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(492, 284315)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Xy[1]==1).sum(),(Xy[1]==0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbcfd6d",
   "metadata": {},
   "source": [
    "# So I think the metric we're focusing on most is Cost, not any traditional scoring method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f6abdb",
   "metadata": {},
   "source": [
    "# Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3ee7a2",
   "metadata": {},
   "source": [
    "### Baseline: Out-of-box Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de94668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "throw(\"Throw-wall to prevent accidental loss of progress\")\n",
    "\n",
    "logreg_oob=LogisticRegression(max_iter=1000)\n",
    "\n",
    "print('Fitting...',end='')\n",
    "logreg_oob.fit(X_train, y_train)\n",
    "\n",
    "print('Scoring Recall...',end='')\n",
    "oob_recall=cross_val_score(logreg_oob, X_train, y_train, cv=5, scoring=\"recall\")\n",
    "print('Scoring Precision...',end='')\n",
    "oob_precision=cross_val_score(logreg_oob, X_train, y_train, cv=5, scoring=\"precision\")\n",
    "print('Predicting Values...')\n",
    "logreg_oob_preds=logreg_oob.predict(X_test)\n",
    "\n",
    "print(f'precision={oob_precision.mean()}\\nrecall={oob_recall.mean()}\\nf1={f1v2(oob_precision.mean(), oob_recall.mean())}')\n",
    "\n",
    "print('Predicting Costs...')\n",
    "logreg_oob_costs=costs(y_test,logreg_oob_preds)\n",
    "print(f'Cost of baseline model: €{logreg_oob_costs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb45d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "costs(y_test,logreg_oob_preds)\n",
    "#Baseline LogReg cost: €0.05034773854644103"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc31f934",
   "metadata": {},
   "source": [
    "### Ok, not terrible, not great. That's an okay baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090c410f",
   "metadata": {},
   "source": [
    "#### Ok after trying an out-of-box baseline I think KNN's going to take way too long to create a proper model let's just stick with logistic regression for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f85e28",
   "metadata": {},
   "source": [
    "# Fine-Tune Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac485c49",
   "metadata": {},
   "source": [
    "### Attempt 1: Undersample Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9cd2e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "RERUN_US_LOGREG=True\n",
    "\n",
    "if RERUN_US_LOGREG:\n",
    "    \n",
    "    us_recall={}\n",
    "    us_precision={}\n",
    "    us_f1={}\n",
    "\n",
    "    us_cost={}\n",
    "    for neg_pos_ratio in range(95,116,1):\n",
    "\n",
    "        \n",
    "\n",
    "        print(neg_pos_ratio, end='_')\n",
    "        \n",
    "        us_precision_temp=0\n",
    "        us_recall_temp=0\n",
    "        us_f1_temp=0\n",
    "        logreg_us_costs=0\n",
    "        \n",
    "        for attempts in range(0,10):\n",
    "            X_tr, X_val, y_tr, y_val=train_test_split(X_train,y_train,test_size=0.25,random_state=(hash(\"Kris Get The Banana Potassium\")*attempts)%(2**32))\n",
    "            \n",
    "            n_pos = np.sum(y_tr == 1)\n",
    "            n_neg = np.sum(y_tr == 0)\n",
    "            sampling_ratio = {1 : n_pos, 0 : n_pos*neg_pos_ratio}\n",
    "            \n",
    "            RUS=imblearn.under_sampling.RandomUnderSampler(sampling_strategy = sampling_ratio, random_state=(hash(\"haha sands uednertal\")*attempts)%(2**32))\n",
    "            X_tr_rs, y_tr_rs = RUS.fit_resample(X_tr, y_tr)\n",
    "\n",
    "            logreg_us=LogisticRegression(max_iter=1000)\n",
    "            logreg_us.fit(X_tr_rs, y_tr_rs)\n",
    "            \n",
    "            #Minimizing cost is my priority - model scoring is secondary\n",
    "            \n",
    "            #print('p_',end='')\n",
    "            #us_precision_temp+=cross_val_score(logreg_us, X_tr_rs, y_tr_rs, cv=5, scoring=\"precision\")    \n",
    "            #print('r_',end='')\n",
    "            #us_recall_temp+=cross_val_score(logreg_us, X_tr_rs, y_tr_rs, cv=5, scoring=\"recall\")\n",
    "            #print('f_',end='')\n",
    "            #us_f1_temp+=cross_val_score(logreg_us, X_tr_rs, y_tr_rs, cv=5, scoring=\"f1\")\n",
    "            \n",
    "            logreg_us_preds=logreg_us.predict(X_val)\n",
    "            logreg_us_costs+=costs(y_val,logreg_us_preds)\n",
    "        \n",
    "        us_recall[neg_pos_ratio]=us_recall_temp/(attempts+1)\n",
    "        us_precision[neg_pos_ratio]=us_precision_temp/(attempts+1)\n",
    "        us_f1[neg_pos_ratio]=us_f1_temp/(attempts+1)\n",
    "\n",
    "        us_cost[neg_pos_ratio]=logreg_us_costs/(attempts+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b773cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterating by 10s: Most efficient sampling ratio is ~1:110\n",
    "# Best undersampling is 1-107 with €0.0421/transaction with validation data (€0.0345 w/ test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbbd17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Increasing data ratio above 40-1 doesn't seem to have much effect on f1, recall, prec...\n",
    "#okay precision gets affected way too much by random chance to make much of a prediction\n",
    "#let's just say ~30-40 is the optimal data ratio for scoring\n",
    "\n",
    "#print('Recall')\n",
    "#for item in us_recall:\n",
    "#    print (item, us_recall[item].mean())\n",
    "\n",
    "# print('Precision')\n",
    "# for item in us_precision:\n",
    "#     print (item, us_precision[item].mean())\n",
    "\n",
    "# print('F1')\n",
    "# for item in us_f1:\n",
    "#     print (item, us_f1[item].mean())\n",
    "\n",
    "print('Cost')\n",
    "for item in us_cost:\n",
    "    print (item, us_cost[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6abc47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pos = np.sum(y_train == 1)\n",
    "n_neg = np.sum(y_train == 0)\n",
    "sampling_ratio = {1 : n_pos, 0 : n_pos*107}\n",
    "\n",
    "RUS=imblearn.under_sampling.RandomUnderSampler(sampling_strategy = sampling_ratio, random_state=(hash(\"haha sands uednertal\"))%(2**32))\n",
    "\n",
    "X_tr_rs, y_tr_rs = RUS.fit_resample(X_train, y_train)\n",
    "\n",
    "logreg_us=LogisticRegression(max_iter=1000)\n",
    "logreg_us.fit(X_tr_rs, y_tr_rs)\n",
    "logreg_us_preds=logreg_us.predict(X_test)\n",
    "costs(y_test,logreg_us_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466d1c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "score(y_test,logreg_us_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee707cc",
   "metadata": {},
   "source": [
    "### Attempt 2: Adjust Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c69ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Fine-tuning weighting hyperparameter\n",
    "\n",
    "RERUN_CW_LOGREG=True\n",
    "\n",
    "if RERUN_CW_LOGREG:\n",
    "\n",
    "    w_recall={}\n",
    "    w_precision={}\n",
    "    w_f1={}\n",
    "\n",
    "    w_cost={}\n",
    "    \n",
    "    #Splitting data train-validation-test 60-20-20\n",
    "    X_tr, X_test, y_tr, y_test=train_test_split(Xy[0],Xy[1],test_size=0.2,random_state=hash(\"HEY EVERY! IT'S ME!\")%(2**32-1))\n",
    "    for neg_pos_ratio in range(70,121,5):\n",
    "        logreg_w_costs=0\n",
    "        print(neg_pos_ratio, end=',')\n",
    "        for attempt in range(0,10):\n",
    "            print(attempt, end='_')\n",
    "            X_train, X_val, y_train, y_val=train_test_split(X_tr,y_tr,test_size=0.25,random_state=hash(\"SPAMT    SPAMTON G. SPAMTON\")*attempt%(2**32-1))\n",
    "            n_pos = np.sum(y_train == 1)\n",
    "            n_neg = np.sum(y_train == 0)\n",
    "            pos_ratio=n_pos/(n_pos+n_neg)\n",
    "            \n",
    "            temp_ratio=pos_ratio*neg_pos_ratio\n",
    "            class_weightings={0:temp_ratio,1:1-temp_ratio}\n",
    "\n",
    "            logreg_w=LogisticRegression(max_iter=1000, class_weight=class_weightings)\n",
    "            logreg_w.fit(X_train, y_train)\n",
    "\n",
    "            #print('p_',end='')\n",
    "            #w_precision_temp=cross_val_score(logreg_w, X_tr_rs, y_tr_rs, cv=5, scoring=\"precision\")\n",
    "            #print('r_',end='')\n",
    "            #w_recall_temp=cross_val_score(logreg_w, X_tr_rs, y_tr_rs, cv=5, scoring=\"recall\")\n",
    "            #print('f_',end='')\n",
    "            #w_f1_temp=cross_val_score(logreg_w, X_tr_rs, y_tr_rs, cv=5, scoring=\"f1\")\n",
    "\n",
    "            #w_recall[neg_pos_ratio]=w_recall_temp\n",
    "            #w_precision[neg_pos_ratio]=w_precision_temp\n",
    "            #w_f1[neg_pos_ratio]=w_f1_temp\n",
    "            \n",
    "            logreg_w_preds=logreg_w.predict(X_val)\n",
    "            logreg_w_costs+=costs(y_val,logreg_w_preds)\n",
    "\n",
    "        w_cost[neg_pos_ratio]=logreg_w_costs/(attempt+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a7f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Recall')\n",
    "#for item in w_recall:\n",
    "#    print (item, w_recall[item].mean())\n",
    "    \n",
    "#print('Precision')\n",
    "#for item in w_precision:\n",
    "#    print (item, w_precision[item].mean())\n",
    "\n",
    "#print('F1')\n",
    "#for item in w_f1:\n",
    "#    print (item, w_f1[item].mean())\n",
    "\n",
    "cost_min=99\n",
    "cost_idx=0\n",
    "\n",
    "print('Cost')\n",
    "for item in w_cost:\n",
    "    print (item, w_cost[item])\n",
    "    if cost_min>w_cost[item]:\n",
    "        cost_idx=item\n",
    "        cost_min=w_cost[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44076608",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best weighting is 90 at €0.0464/transaction with validation data (€0.0266 w/test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce53384",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_test, y_tr, y_test=train_test_split(Xy[0],Xy[1],test_size=0.2,random_state=hash(\"HEY EVERY! IT'S ME!\")%(2**32-1))\n",
    "X_train, X_val, y_train, y_val=train_test_split(X_tr,y_tr,test_size=0.25,random_state=hash(\"SPAMT    SPAMTON G. SPAMTON\")%(2**32-1))\n",
    "n_pos = np.sum(y_train == 1)\n",
    "n_neg = np.sum(y_train == 0)\n",
    "pos_ratio=n_pos/(n_pos+n_neg)\n",
    "\n",
    "temp_ratio=pos_ratio*90\n",
    "class_weightings={0:temp_ratio,1:1-temp_ratio}\n",
    "logreg_w=LogisticRegression(max_iter=1000, class_weight=class_weightings)\n",
    "logreg_w.fit(X_train, y_train)\n",
    "logreg_w_preds=logreg_w.predict(X_test)\n",
    "costs(y_test,logreg_w_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ad7f5a",
   "metadata": {},
   "source": [
    "### Attempt 3: Adjust Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c89d849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "\n",
    "RERUN_TH_LOGREG=True\n",
    "\n",
    "\n",
    "if RERUN_TH_LOGREG:\n",
    "    th_recall={}\n",
    "    th_precision={}\n",
    "    th_f1={}\n",
    "\n",
    "    th_cost={}\n",
    "\n",
    "    #Don't think I can use validation here because there's no custom threshold for logreg\n",
    "    #THAT'S NOT VERY [[BIG SHOT]] OF YOU, [[SciKit]].\n",
    "    #but I'll randomize the train/holdout data and average it\n",
    "    #a lot\n",
    "    \n",
    "    for i in range(0,51):\n",
    "        th_cost[i]=[]\n",
    "        th_recall[i]=[]\n",
    "        th_precision[i]=[]\n",
    "        th_f1[i]=[]\n",
    "        \n",
    "    for attempt in range(0,10):\n",
    "        print(attempt,end='_')\n",
    "        X_train, X_test, y_train, y_test=train_test_split(Xy[0],Xy[1],test_size=0.2,random_state=hash(\"Chicken Dance\")%(2**32-1))\n",
    "        X_train, X_val, y_train, y_val=train_test_split(X_train,y_train,test_size=0.25,random_state=hash(\"Take On Me\")*attempt%(2**32-1))\n",
    "        \n",
    "        logreg_th=LogisticRegression(max_iter=1000)\n",
    "        logreg_th.fit(X_train, y_train)\n",
    "        \n",
    "        logreg_th_probs=logreg_th.predict_proba(X_val)\n",
    "        \n",
    "        for thresh in range(1,51):\n",
    "            logreg_th_preds = np.where(logreg_th_probs[:,1] > (thresh*0.01), 1, 0)\n",
    "            logreg_th_costs=costs(y_val,logreg_th_preds)\n",
    "            \n",
    "            scores=score(y_val,logreg_th_preds)\n",
    "            \n",
    "            th_recall[thresh].append(scores['recall'])\n",
    "            th_precision[thresh].append(scores['precision'])\n",
    "            th_f1[thresh].append(scores['f1'])\n",
    "            \n",
    "            th_cost[thresh].append(logreg_th_costs)\n",
    "    print('done')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a111827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for thresh in th_cost:\n",
    "    print(thresh,sum(th_cost[thresh])/10)\n",
    "    \n",
    "#Best threshold for cost: 20-30% (23% best) €0.0658/transaction (€0.0561 with validation data @ 21%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d5b69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_th=LogisticRegression(max_iter=1000)\n",
    "logreg_th.fit(X_train, y_train)\n",
    "logreg_th_probs=logreg_th.predict_proba(X_test)\n",
    "logreg_th_preds = np.where(logreg_th_probs[:,1] > 0.23, 1, 0)\n",
    "logreg_th_costs=costs(y_test,logreg_th_preds)\n",
    "logreg_th_costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf73700",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Model Type 2: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aa4789",
   "metadata": {},
   "source": [
    "### Baseline RF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cd7871b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mk/f8dyn74n2kb1xh98q8xhy3d80000gn/T/ipykernel_69066/1748849736.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrf_oob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrf_oob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mrf_oob_probs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrf_oob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/metis/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[1;32m    388\u001b[0m                              \u001b[0;34m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'threads'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                 delayed(_parallel_build_trees)(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/metis/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/metis/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/metis/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/metis/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/metis/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/metis/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/metis/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/metis/lib/python3.8/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/metis/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    167\u001b[0m                                                         indices=indices)\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/metis/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \"\"\"\n\u001b[1;32m   1251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1253\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/metis/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    392\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Xy[0], Xy[1], test_size=0.2, random_state=hash(\"I CAN DO ANYTHING\")%(2**32))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=hash(\"CHAOS, CHAOS\")%(2**32))\n",
    "\n",
    "rf_oob = RandomForestRegressor(max_features=3)\n",
    "rf_oob.fit(X_train,y_train)\n",
    "\n",
    "rf_oob_probs=rf_oob.predict(X_val)\n",
    "\n",
    "thresh=50\n",
    "rf_oob_preds=np.where(rf_oob_probs > (thresh*0.01), True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5e2b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "costs(y_val,rf_oob_preds)\n",
    "# €0.0489\n",
    "# Ok that's actually really good for a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e9766d",
   "metadata": {},
   "source": [
    "### Let's tune max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0033c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Testing max_features\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xy[0], Xy[1], test_size=0.2, random_state=hash(\"I CAN DO ANYTHING\")%(2**32))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=hash(\"CHAOS, CHAOS\")%(2**32))\n",
    "\n",
    "probs={}\n",
    "\n",
    "for f in range(1,11):\n",
    "    print(f,end=',')\n",
    "    rf_f = RandomForestRegressor(max_features=f)\n",
    "    rf_f.fit(X_train,y_train)\n",
    "\n",
    "    rf_f_probs=rf_f.predict(X_val)\n",
    "    \n",
    "    probs[f]=rf_f_probs\n",
    "    \n",
    "    \n",
    "print('done')    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382176e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in probs:\n",
    "    thresh=50\n",
    "    rf_f_preds=np.where(probs[item] > (thresh*0.01), True, False)\n",
    "    print(item,costs(y_val,rf_f_preds))\n",
    "    \n",
    "#Balancing time and cost: Let's stick with 3 features because after that it really doesn't seem to matter that much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28ce54a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_features=3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_f = RandomForestRegressor(max_features=3)\n",
    "rf_f.fit(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1284499",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_temp=rf_f.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaf29fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_temp=rf_f.predict(X_val)\n",
    "thresh_tuning=[]\n",
    "for thresh in range(0,26):\n",
    "    rf_f_preds=np.where(probs_temp > (thresh*0.01), True, False)\n",
    "    thresh_tuning.append(costs(y_val,rf_f_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b2a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best threshold is 0.18, with cost of €0.0385 with validation data (€0.0348 for test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb64bb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_temp=rf_f.predict(X_test)\n",
    "thresh_tuning1=[]\n",
    "for thresh in range(0,26):\n",
    "    rf_f_preds=np.where(probs_temp > (thresh*0.01), True, False)\n",
    "    thresh_tuning1.append(costs(y_test,rf_f_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e872b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "score(y_test,rf_f_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c44bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_tuning[18]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea2d0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_tuning1[18]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37359052",
   "metadata": {},
   "source": [
    "# Model Type 3: Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911b4f99",
   "metadata": {},
   "source": [
    "### Almost-out-of-box model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8d70f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data train-validation-test 60-20-20\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xy[0], Xy[1], test_size=0.2, random_state=hash(\"I Am Going To Touch The Cheese\")%(2**32))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=hash(\"Something About TerminalMontage\")%(2**32))\n",
    "\n",
    "#Modified code from XGB notebook\n",
    "gb_oob = xgb.XGBRegressor(random_state=hash(\"nope.avi\")%(2**32))\n",
    "eval_set = [(X_train, y_train), (X_val, y_val)]\n",
    "\n",
    "gb_oob_fit = gb_oob.fit(X_train,y_train,eval_set=eval_set,verbose=False)\n",
    "\n",
    "gb_oob_proba=gb_oob.predict(X_val)\n",
    "gb_oob_preds=np.where(gb_oob_proba > 0.5, True, False)\n",
    "\n",
    "print(\"Validation Data Cost\",costs(y_val,gb_oob_preds))\n",
    "\n",
    "gb_oob_proba=gb_oob.predict(X_test)\n",
    "gb_oob_preds=np.where(gb_oob_proba > 0.5, True, False)\n",
    "print(\"Test Data Cost\",costs(y_test,gb_oob_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688549e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Data Cost 0.03639916704838485\n",
    "# Test Data Cost 0.04491577667942599"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d0e73b",
   "metadata": {},
   "source": [
    "### Not a bad baseline; let's improve on that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d4022c",
   "metadata": {},
   "source": [
    "### Attempt 1: Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d49c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_us_costs={}\n",
    "\n",
    "for pos_neg_ratio in range(0,150,10):\n",
    "    print(pos_neg_ratio,end=',')\n",
    "    n_pos = np.sum(y_train == 1)\n",
    "    sampling_ratio = {1 : n_pos, 0 : n_pos*pos_neg_ratio}\n",
    "\n",
    "    RUS=imblearn.under_sampling.RandomUnderSampler(sampling_strategy = sampling_ratio, random_state=(hash(\"haha python go b+r*10\"))%(2**32))\n",
    "    X_tr_rs, y_tr_rs = RUS.fit_resample(X_train, y_train)\n",
    "\n",
    "    gb_us = xgb.XGBRegressor(random_state=hash(\"nope.avi\")%(2**32))\n",
    "    eval_set = [(X_tr_rs, y_tr_rs), (X_val, y_val)]\n",
    "\n",
    "    gb_us_fit = gb_us.fit(X_tr_rs,y_tr_rs,eval_set=eval_set, verbose=False)\n",
    "    gb_us_proba=gb_us_fit.predict(X_val)\n",
    "    gb_us_preds=np.where(gb_us_proba > 0.5, True, False)\n",
    "\n",
    "    gb_us_costs[pos_neg_ratio]=costs(y_val,gb_us_preds)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2bd407",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_pos = np.sum(y_train == 1)\n",
    "sampling_ratio = {1 : n_pos, 0 : n_pos*80}\n",
    "\n",
    "RUS=imblearn.under_sampling.RandomUnderSampler(sampling_strategy = sampling_ratio, random_state=(hash(\"haha python go b+r*10\"))%(2**32))\n",
    "X_tr_rs, y_tr_rs = RUS.fit_resample(X_train, y_train)\n",
    "\n",
    "gb_us = xgb.XGBRegressor(random_state=hash(\"nope.avi\")%(2**32))\n",
    "eval_set = [(X_tr_rs, y_tr_rs), (X_val, y_val)]\n",
    "\n",
    "gb_us_fit = gb_us.fit(X_tr_rs,y_tr_rs,eval_set=eval_set, verbose=False)\n",
    "gb_us_proba=gb_us_fit.predict(X_test)\n",
    "gb_us_preds=np.where(gb_us_proba > 0.5, True, False)\n",
    "\n",
    "costs(y_test,gb_us_preds)\n",
    "\n",
    "    \n",
    "#Above ratio 1:100, there isn't much difference so let's just use 1:80 for training it's just faster\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9380853",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sampling_ratio = {1 : n_pos, 0 : n_pos*80}\n",
    "\n",
    "RUS=imblearn.under_sampling.RandomUnderSampler(sampling_strategy = sampling_ratio, random_state=(hash(\"haha python go b+r*10\"))%(2**32))\n",
    "X_tr_rs, y_tr_rs = RUS.fit_resample(X_train, y_train)\n",
    "\n",
    "gb_us = xgb.XGBRegressor(random_state=hash(\"nope.avi\")%(2**32))\n",
    "eval_set = [(X_tr_rs, y_tr_rs), (X_test, y_test)]\n",
    "\n",
    "gb_us_fit = gb_us.fit(X_tr_rs,y_tr_rs,eval_set=eval_set, verbose=False)\n",
    "gb_us_proba=gb_us_fit.predict(X_test)\n",
    "gb_us_preds=np.where(gb_us_proba > 0.5, True, False)\n",
    "\n",
    "costs(y_test,gb_us_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c74ebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best ratio 1:80 @ €0.0463/transaction in test data; €0.0381 for validation\n",
    "#Probably a bit much overfitting; we'll work on that next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5ff9c4",
   "metadata": {},
   "source": [
    "### Attempt 2: General re-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dd2c2f",
   "metadata": {},
   "source": [
    "##### Step 1 - Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6fb58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pos = np.sum(y_train == 1)\n",
    "sampling_ratio = {1 : n_pos, 0 : n_pos*80}\n",
    "RUS=imblearn.under_sampling.RandomUnderSampler(sampling_strategy = sampling_ratio, random_state=(hash(\"haha python go b+r*10\"))%(2**32))\n",
    "X_tr_rs, y_tr_rs = RUS.fit_resample(X_train, y_train)\n",
    "\n",
    "cost_t={}\n",
    "cost_tt={}\n",
    "\n",
    "for lr in range(1,31):\n",
    "    print(lr,end='_')\n",
    "    gb_t = xgb.XGBRegressor(\n",
    "        #max_depth=7,\n",
    "        learning_rate=lr*0.01,\n",
    "        #subsample=0.8,\n",
    "        #min_child_weight=12,\n",
    "        #colsample_bytree=.7,\n",
    "        random_state=hash(\"nope.avi\")%(2**32))\n",
    "\n",
    "    eval_set = [(X_tr_rs, y_tr_rs), (X_val, y_val)]\n",
    "\n",
    "    gb_t_fit = gb_t.fit(X_tr_rs,y_tr_rs,eval_set=eval_set, verbose=False)\n",
    "    gb_t_proba=gb_t.predict(X_val)\n",
    "    gb_t_preds=np.where(gb_t_proba > 0.5, True, False)\n",
    "    \n",
    "    gb_tt_proba=gb_t.predict(X_test)\n",
    "    gb_tt_preds=np.where(gb_tt_proba > 0.5, True, False)\n",
    "    \n",
    "    cost_t[lr]=costs(y_val,gb_t_preds)\n",
    "    cost_tt[lr]=costs(y_test,gb_tt_preds)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dda5fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for i in cost_t:\n",
    "#    print(i,cost_t[i])\n",
    "    \n",
    "#0.1 appears to be the best learning_rate in general"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeed28e",
   "metadata": {},
   "source": [
    "##### Step 2 - max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b582d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pos = np.sum(y_train == 1)\n",
    "sampling_ratio = {1 : n_pos, 0 : n_pos*80}\n",
    "RUS=imblearn.under_sampling.RandomUnderSampler(sampling_strategy = sampling_ratio, random_state=(hash(\"haha python go b+r*10\"))%(2**32))\n",
    "X_tr_rs, y_tr_rs = RUS.fit_resample(X_train, y_train)\n",
    "\n",
    "cost_t={}\n",
    "cost_tt={}\n",
    "\n",
    "for md in range(1,11):\n",
    "    print(md,end='_')\n",
    "    gb_t = xgb.XGBRegressor(\n",
    "        max_depth=md,\n",
    "        learning_rate=0.1,\n",
    "        #subsample=0.1,\n",
    "        #min_child_weight=12,\n",
    "        #colsample_bytree=.7,\n",
    "        random_state=hash(\"nope.avi\")%(2**32))\n",
    "\n",
    "\n",
    "    eval_set = [(X_tr_rs, y_tr_rs), (X_val, y_val)]\n",
    "\n",
    "    gb_t_fit = gb_t.fit(X_tr_rs,y_tr_rs,eval_set=eval_set, verbose=False)\n",
    "    gb_t_proba=gb_t.predict(X_val)\n",
    "    gb_t_preds=np.where(gb_t_proba > 0.5, True, False)\n",
    "    \n",
    "    gb_tt_proba=gb_t.predict(X_test)\n",
    "    gb_tt_preds=np.where(gb_tt_proba > 0.5, True, False)\n",
    "    \n",
    "    cost_t[md]=costs(y_val,gb_t_preds)\n",
    "    cost_tt[md]=costs(y_test,gb_tt_preds)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65723c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cost_t:\n",
    "    print(i,cost_t[i])\n",
    "#5 is best max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5596763",
   "metadata": {},
   "source": [
    "##### Step 3 - min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6ac359",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pos = np.sum(y_train == 1)\n",
    "sampling_ratio = {1 : n_pos, 0 : n_pos*80}\n",
    "RUS=imblearn.under_sampling.RandomUnderSampler(sampling_strategy = sampling_ratio, random_state=(hash(\"haha python go b+r*10\"))%(2**32))\n",
    "X_tr_rs, y_tr_rs = RUS.fit_resample(X_train, y_train)\n",
    "\n",
    "cost_t={}\n",
    "cost_tt={}\n",
    "\n",
    "for mcw in range(2,52,2):\n",
    "    print(mcw,end='_')\n",
    "    gb_t = xgb.XGBRegressor(\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        #subsample=0.1,\n",
    "        min_child_weight=mcw,\n",
    "        #colsample_bytree=.7,\n",
    "        random_state=hash(\"nope.avi\")%(2**32))\n",
    "    \n",
    "    eval_set = [(X_tr_rs, y_tr_rs), (X_val, y_val)]\n",
    "\n",
    "    gb_t_fit = gb_t.fit(X_tr_rs,y_tr_rs,eval_set=eval_set, verbose=False)\n",
    "    gb_t_proba=gb_t.predict(X_val)\n",
    "    gb_t_preds=np.where(gb_t_proba > 0.5, True, False)\n",
    "    \n",
    "    gb_tt_proba=gb_t.predict(X_test)\n",
    "    gb_tt_preds=np.where(gb_tt_proba > 0.5, True, False)\n",
    "    \n",
    "    cost_t[mcw]=costs(y_val,gb_t_preds)\n",
    "    cost_tt[mcw]=costs(y_test,gb_tt_preds)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be7ac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cost_t:\n",
    "    print(i,cost_t[i])\n",
    "#>20 is best min_child_weight. Doesn't seem to increase past that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43adeafa",
   "metadata": {},
   "source": [
    "##### Step 4 -colsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73da37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pos = np.sum(y_train == 1)\n",
    "sampling_ratio = {1 : n_pos, 0 : n_pos*80}\n",
    "RUS=imblearn.under_sampling.RandomUnderSampler(sampling_strategy = sampling_ratio, random_state=(hash(\"haha python go b+r*10\"))%(2**32))\n",
    "X_tr_rs, y_tr_rs = RUS.fit_resample(X_train, y_train)\n",
    "\n",
    "cost_t={}\n",
    "cost_tt={}\n",
    "\n",
    "for ct in range(2,102,2):\n",
    "    print(ct,end='_')\n",
    "    gb_t = xgb.XGBRegressor(\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        #subsample=0.1,\n",
    "        min_child_weight=20,\n",
    "        colsample_bytree=ct*0.01,\n",
    "        random_state=hash(\"nope.avi\")%(2**32))\n",
    "    \n",
    "    eval_set = [(X_tr_rs, y_tr_rs), (X_val, y_val)]\n",
    "\n",
    "    gb_t_fit = gb_t.fit(X_tr_rs,y_tr_rs,eval_set=eval_set, verbose=False)\n",
    "    gb_t_proba=gb_t.predict(X_val)\n",
    "    gb_t_preds=np.where(gb_t_proba > 0.5, True, False)\n",
    "    \n",
    "    gb_tt_proba=gb_t.predict(X_test)\n",
    "    gb_tt_preds=np.where(gb_tt_proba > 0.5, True, False)\n",
    "    \n",
    "    cost_t[ct]=costs(y_val,gb_t_preds)\n",
    "    cost_tt[ct]=costs(y_test,gb_tt_preds)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437b6f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cost_t:\n",
    "    print(i,cost_t[i])\n",
    "#>.30 is best colsample. Doesn't seem to increase past that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07b61b8",
   "metadata": {},
   "source": [
    "##### Step 5: Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf0c02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pos = np.sum(y_train == 1)\n",
    "sampling_ratio = {1 : n_pos, 0 : n_pos*80}\n",
    "RUS=imblearn.under_sampling.RandomUnderSampler(sampling_strategy = sampling_ratio, random_state=(hash(\"haha python go b+r*10\"))%(2**32))\n",
    "X_tr_rs, y_tr_rs = RUS.fit_resample(X_train, y_train)\n",
    "\n",
    "cost_t={}\n",
    "cost_tt={}\n",
    "\n",
    "for ss in range(5,105,5):\n",
    "    print(ss,end='_')\n",
    "    gb_t = xgb.XGBRegressor(\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        subsample=ss*0.01,\n",
    "        min_child_weight=20,\n",
    "        colsample_bytree=0.3,\n",
    "        random_state=hash(\"nope.avi\")%(2**32))\n",
    "\n",
    "    \n",
    "    eval_set = [(X_tr_rs, y_tr_rs), (X_val, y_val)]\n",
    "\n",
    "    gb_t_fit = gb_t.fit(X_tr_rs,y_tr_rs,eval_set=eval_set, verbose=False)\n",
    "    gb_t_proba=gb_t.predict(X_val)\n",
    "    gb_t_preds=np.where(gb_t_proba > 0.5, True, False)\n",
    "    \n",
    "    gb_tt_proba=gb_t.predict(X_test)\n",
    "    gb_tt_preds=np.where(gb_tt_proba > 0.5, True, False)\n",
    "    \n",
    "    cost_t[ss]=costs(y_val,gb_t_preds)\n",
    "    cost_tt[ss]=costs(y_test,gb_tt_preds)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45779def",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cost_t:\n",
    "    print(i,cost_t[i])\n",
    "    \n",
    "for i in cost_tt:\n",
    "    print(i,cost_tt[i])\n",
    "#~.40 appears to be best subsample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e9f0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pos = np.sum(y_train == 1)\n",
    "sampling_ratio = {1 : n_pos, 0 : n_pos*80}\n",
    "RUS=imblearn.under_sampling.RandomUnderSampler(sampling_strategy = sampling_ratio, random_state=(hash(\"haha python go b+r*10\"))%(2**32))\n",
    "X_tr_rs, y_tr_rs = RUS.fit_resample(X_train, y_train)\n",
    "\n",
    "cost_t={}\n",
    "cost_tt={}\n",
    "\n",
    "for thresh in range(28,52,2):\n",
    "    print(thresh,end='_')\n",
    "    gb_t = xgb.XGBRegressor(\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.45,\n",
    "        min_child_weight=20,\n",
    "        colsample_bytree=.25,\n",
    "        random_state=hash(\"nope.avi\")%(2**32))\n",
    "\n",
    "    n_pos = np.sum(y_train == 1)\n",
    "    n_neg = np.sum(y_train == 0)\n",
    "    sampling_ratio = {1 : n_pos, 0 : n_pos*80}\n",
    "    \n",
    "    eval_set = [(X_tr_rs, y_tr_rs), (X_val, y_val)]\n",
    "\n",
    "    gb_t_fit = gb_t.fit(X_tr_rs,y_tr_rs,eval_set=eval_set, verbose=False)\n",
    "    gb_t_proba=gb_t.predict(X_val)\n",
    "    gb_t_preds=np.where(gb_t_proba > thresh*0.01, True, False)\n",
    "    \n",
    "    gb_tt_proba=gb_t.predict(X_test)\n",
    "    gb_tt_preds=np.where(gb_tt_proba > thresh*0.01, True, False)\n",
    "    \n",
    "    cost_t[thresh]=costs(y_val,gb_t_preds)\n",
    "    cost_tt[thresh]=costs(y_test,gb_tt_preds)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b70479",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cost_t:\n",
    "    print(i,cost_t[i]-cost_tt[i])\n",
    "\n",
    "#cost_tt\n",
    "#>0.38 thresh seems to have the best val/test cost of 0.0327/0.0363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701b8307",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13283572",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_t = xgb.XGBRegressor(\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.45,\n",
    "    min_child_weight=20,\n",
    "    colsample_bytree=.25,\n",
    "    random_state=hash(\"nope.avi\")%(2**32))\n",
    "\n",
    "n_pos = np.sum(y_train == 1)\n",
    "n_neg = np.sum(y_train == 0)\n",
    "sampling_ratio = {1 : n_pos, 0 : n_pos*80}\n",
    "\n",
    "eval_set = [(X_tr_rs, y_tr_rs), (X_val, y_val)]\n",
    "\n",
    "gb_t_fit = gb_t.fit(X_tr_rs,y_tr_rs,eval_set=eval_set, verbose=False)\n",
    "gb_t_proba=gb_t.predict(X_val)\n",
    "gb_t_preds=np.where(gb_t_proba > 0.30, True, False)\n",
    "\n",
    "gb_tt_proba=gb_t.predict(X_test)\n",
    "gb_tt_preds=np.where(gb_tt_proba > 0.30, True, False)\n",
    "\n",
    "cost_t=costs(y_val,gb_t_preds)\n",
    "cost_tt=costs(y_test,gb_tt_preds)\n",
    "\n",
    "score(y_test,gb_tt_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341ff442",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f204089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
